<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ian Kilgore's blog</title><link href="http://blog.iank.org/" rel="alternate"></link><link href="http://blog.iank.org/feeds/math.atom.xml" rel="self"></link><id>http://blog.iank.org/</id><updated>2015-07-26T02:11:00-04:00</updated><entry><title>Reconstructing phase space and estimating maximal Lyapunov exponent from experimental time series</title><link href="http://blog.iank.org/reconstructing-phase-space-and-estimating-maximal-lyapunov-exponent-from-experimental-time-series.html" rel="alternate"></link><updated>2015-07-26T02:11:00-04:00</updated><author><name>Ian Kilgore</name></author><id>tag:blog.iank.org,2015-07-26:reconstructing-phase-space-and-estimating-maximal-lyapunov-exponent-from-experimental-time-series.html</id><summary type="html">&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;Last week I took some measurements of a system for my research and needed to show if the system was &lt;a href="https://en.wikipedia.org/wiki/Chaos_theory"&gt;chaotic&lt;/a&gt;. The measured data was a 1-dimensional time series from a &lt;a href="http://www.polytec.com/us/products/vibration-sensors/single-point-vibrometers/complete-systems/pdv-100-portable-digital-vibrometer/"&gt;Laser Doppler Vibrometer (LDV)&lt;/a&gt;. In order to show the system was chaotic I reconstructed state space using the method of delays, and estimated the maximal Lyapunov exponent of the system.&lt;/p&gt;
&lt;p&gt;Continuous&lt;sup id="fnref:discrete"&gt;&lt;a class="footnote-ref" href="#fn:discrete" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; systems must be at least three-dimensional in order to exhibit chaos, but it's possible to reconstruct higher-dimensional state space from a 1-dimensional time series[&lt;sup id="fnref:packard"&gt;&lt;a class="footnote-ref" href="#fn:packard" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] by lagging the data, e.g. taking $x(t)$ and turning it into a series of vectors $[ x(t), x(t+T), ..., x(t+nT) ]$. There are a variety of methods for determining the constant $T$, and the embedding dimension (sometimes called $m$), and there is no clear best method. Fortunately these systems are somewhat forgiving.&lt;/p&gt;
&lt;h3&gt;Method of delays&lt;/h3&gt;
&lt;p&gt;In my experiment I generated a time series based on the &lt;a href="https://en.wikipedia.org/wiki/Lorenz_system"&gt;Lorenz system&lt;/a&gt; by integrating the Lorenz equations with one of MATLAB's Runge-Kutta ODE solvers (code &lt;a href="https://github.com/iank/lyapunov_estimation/blob/master/lorenz_ode.m"&gt;lorenz_ode.m&lt;/a&gt; &lt;a href="https://github.com/iank/lyapunov_estimation/blob/master/gen_chaos.m"&gt;gen_chaos.m&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Plotting $x$, $y$, and $z$ over $t=[0,100]$ produces a nice Lorenz attractor&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lorenz attractor" src="/images/chaos/lorenz_attractor.png" /&gt;&lt;/p&gt;
&lt;p&gt;I took the $z$ variable (&lt;a href="https://raw.githubusercontent.com/iank/lyapunov_estimation/master/lorenz_z.txt"&gt;lorenz_z.txt&lt;/a&gt;) and used the method of delays to reproduce the attractor. I just guessed at a lag value. I'll demonstrate a better method below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Reproduced attractor" src="/images/chaos/lorenz_ts_reconstruction.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that it is warped, but the underlying structure is there. For my experiment I generated a 100 Hz sinusoidal tone and added a scaled (1/50) and sped-up (100x) version of the Lorenz time series generated above. One second of data (at 204,800 samples/sec) was sent to a DAC, amplified, and used to drive a speaker coil. The vibration of the coil was measured using an LDV, producing a &lt;a href="https://raw.githubusercontent.com/iank/lyapunov_estimation/master/lorenz_ldv.csv"&gt;time series&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Time series LDV data" src="/images/chaos/lorenz_ldv.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Determining lag&lt;/h3&gt;
&lt;p&gt;Andrew Fraser and Harry Swinney[&lt;sup id="fnref:fraser"&gt;&lt;a class="footnote-ref" href="#fn:fraser" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] give a method for determining appropriate lag for the method of delays using mutual information&lt;sup id="fnref:memory"&gt;&lt;a class="footnote-ref" href="#fn:memory" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;. &lt;a href="http://www.physics.emory.edu/faculty/weeks/"&gt;Dr. Eric Weeks&lt;/a&gt; wrote a &lt;a href="http://www.physics.emory.edu/faculty/weeks//software/minfo.html"&gt;C program&lt;/a&gt; which implements their method, and you can &lt;a href="http://www.physics.emory.edu/faculty/weeks//research/tseries3.html"&gt;read more about it on his website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Running this program on my LDV data produced &lt;a href="https://raw.githubusercontent.com/iank/lyapunov_estimation/master/lorenz_ldv.mi.csv"&gt;this signal&lt;/a&gt;, plotted below. From this we can see that the first minimum is around $T=770$ (discrete steps, or $770 / Fs = 3.76$ milliseconds).&lt;/p&gt;
&lt;p&gt;&lt;img alt="I(T) for LDV data" src="/images/chaos/lorenz_ldv_mi.png" /&gt;&lt;/p&gt;
&lt;p&gt;I reconstruct the attractor in MATLAB using this delay:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;lz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;csvread&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lorenz_ldv.csv&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="nt"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;length&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;lz&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="nt"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;770&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;lz&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="nx"&gt;lz&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;lz&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;X&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="nd"&gt;:100000&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="o"&gt;:);&lt;/span&gt;  &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;take&lt;/span&gt; &lt;span class="nt"&gt;first&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="nt"&gt;half&lt;/span&gt; &lt;span class="nt"&gt;of&lt;/span&gt; &lt;span class="nt"&gt;data&lt;/span&gt; &lt;span class="nt"&gt;so&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;plot&lt;/span&gt; &lt;span class="nt"&gt;isn&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t too dense&lt;/span&gt;
&lt;span class="s1"&gt;plot(X(:,1), X(:,2));&lt;/span&gt;
&lt;span class="s1"&gt;title(&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;Reconstructed&lt;/span&gt; &lt;span class="nt"&gt;attractor&lt;/span&gt; &lt;span class="nt"&gt;from&lt;/span&gt; &lt;span class="nt"&gt;LDV&lt;/span&gt; &lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;);&lt;/span&gt;
&lt;span class="s1"&gt;xlabel(&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;t&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;); ylabel(&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;x&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;770&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Reconstructed attractor from LDV data" src="/images/chaos/lorenz_ldv_attractor.png" /&gt;&lt;/p&gt;
&lt;p&gt;This does not look like the Lorenz attractor because the system is dominated by the 100 Hz carrier. It may be possible to get a better-looking reconstruction by taking the envelope of the signal. Zooming, we see dense orbits&lt;sup id="fnref:control"&gt;&lt;a class="footnote-ref" href="#fn:control" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Reconstructed attractor from LDV data" src="/images/chaos/lorenz_ldv_attractor_zoom.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Estimation of MLE&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Lyapunov_exponent"&gt;Lyapunov exponents&lt;/a&gt; describe how a system expands and contracts in phase space. There is a spectrum of exponents but the maximal Lyapunov exponent (MLE, often written $\lambda_1$) characterizes the system. One of the features of chaos is exponential divergence (sensitivity to initial conditions). Two trajectories, initially arbitrarily close to each other, will diverge exponentially in phase space. The existence of a positive Lyapunov exponent is good evidence for chaos. It is also an indication of the long-term predictibility of a system: it may be specified in &lt;a href="https://en.wikipedia.org/wiki/Nat_(unit)"&gt;nats&lt;/a&gt; per second (or bits or digits), giving the amount of time it takes for uncertainty in a system to increase by a factor of $e$ (or 2 or 10). Note that there must be &lt;em&gt;exponential&lt;/em&gt; divergence for this analysis to be meaningful.&lt;/p&gt;
&lt;p&gt;If differential equations for a system are known, it may be possible to solve for $\lambda_1$. Otherwise, it can be estimated by solving the system numerically, as in [&lt;sup id="fnref:benettin"&gt;&lt;a class="footnote-ref" href="#fn:benettin" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;]: Integrate the system for two nearby initial conditions and watch how the trajectories diverge. Renormalization is necessary as most systems are bounded (two points can only be so far away from each other in phase space).&lt;/p&gt;
&lt;p&gt;In my case, I have a small amount of data (204800 samples), and only one trajectory in phase space. Rosenstein, et. al. &lt;sup id="fnref:rosenstein"&gt;&lt;a class="footnote-ref" href="#fn:rosenstein" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt; present a method for estimating $\lambda_1$ in this case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reconstruct the attractor using the method of delays (I use an embedding dimension of $m=10$)&lt;/li&gt;
&lt;li&gt;For each point $j$:&lt;ul&gt;
&lt;li&gt;Find that point's nearest neighbor with the constraint that it must have at least one mean period of temporal separation. The temporal separation constraint allows us to treat our single trajectory as a collection of trajectories having separate evolutions.&lt;/li&gt;
&lt;li&gt;Follow the evolution of both points, calculating the distance $d_j(i)$ between them with respect to time step $i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Take the average over all points $j$: $d(i) = \mathrm{mean}(d_j(i))$&lt;/li&gt;
&lt;li&gt;Plot $ln(d(i))$. A straight line here indicates exponential divergence (there will likely be two regions, an initial exponential divergence and then a flat portion. This happens when phase space is bounded and there is a maximum distance, as discussed above).&lt;/li&gt;
&lt;li&gt;Fit a line to $ln(d(i))$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I've &lt;a href="https://github.com/iank/lyapunov_estimation/blob/master/rosenstein.m"&gt;implemented this algorithm in MATLAB&lt;/a&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;di&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;rosenstein&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;lz&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;770&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="nt"&gt;Fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;204800&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;((&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="nd"&gt;:length&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;di&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nt"&gt;Fs&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;log&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;di&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;span class="nt"&gt;title&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Divergence for LDV data - Average distance between nearest neighbors&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="nt"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Lag (s)&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="nt"&gt;ylabel&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ln(d_j(i))&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

&lt;span class="o"&gt;%%&lt;/span&gt; &lt;span class="nt"&gt;Fit&lt;/span&gt; &lt;span class="nt"&gt;line&lt;/span&gt;
&lt;span class="nt"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;6643&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;polyfit&lt;/span&gt;&lt;span class="o"&gt;((&lt;/span&gt;&lt;span class="nt"&gt;x1&lt;/span&gt;&lt;span class="nd"&gt;:x2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/Fs, log(di(x1:x2)),1);&lt;/span&gt;
&lt;span class="s1"&gt;h = refline(p);&lt;/span&gt;
&lt;span class="s1"&gt;set(h, &amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;LineStyle&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;);&lt;/span&gt;
&lt;span class="s1"&gt;text(0.04, 3.4, sprintf(&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nc"&gt;.2f&lt;/span&gt; &lt;span class="nt"&gt;nats&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;sec&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Divergence of LDV data" src="/images/chaos/lorenz_ldv_divergence.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Sanity check&lt;/h3&gt;
&lt;p&gt;Is this a reasonable value for our system? I used this method on the original, computed&lt;sup id="fnref:computed"&gt;&lt;a class="footnote-ref" href="#fn:computed" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt; Lorenz data ($[x(t), y(t), z(t)]$) and the computed time series $[z(t), z(t+T), ..., z(t+9T)]$ .&lt;/p&gt;
&lt;p&gt;&lt;img alt="Divergence of original attractor" src="/images/chaos/lorenz_orig_divergence.png" /&gt;
&lt;img alt="Divergence of z(t) time series" src="/images/chaos/lorenz_z_divergence.png" /&gt;&lt;/p&gt;
&lt;p&gt;Note that the $z(t)$ data has been sped up by a factor of 100, so the estimated values for $\lambda_1$ are about equal. The &lt;a href="http://sprott.physics.wisc.edu/chaos/lorenzle.htm"&gt;actual MLE for the Lorenz system&lt;/a&gt; is known to be about 0.9056. So there is a significant error&lt;sup id="fnref:error"&gt;&lt;a class="footnote-ref" href="#fn:error" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt; but the values are within reason for my purpose, which is to broadly characterize a time series.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:discrete"&gt;
&lt;p&gt;It's possible to have e.g. 1-D discrete maps which are chaotic.&amp;#160;&lt;a class="footnote-backref" href="#fnref:discrete" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:packard"&gt;
&lt;p&gt;Packard, Norman H., et al. "Geometry from a time series." Physical review letters 45.9 (1980): 712.&amp;#160;&lt;a class="footnote-backref" href="#fnref:packard" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:fraser"&gt;
&lt;p&gt;Fraser, Andrew M., and Harry L. Swinney. "Independent coordinates for strange attractors from mutual information." Physical review A 33.2 (1986): 1134.&amp;#160;&lt;a class="footnote-backref" href="#fnref:fraser" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:memory"&gt;
&lt;p&gt;One of the things that differentiates chaos from random noise is that there is long-term memory, i.e. the autocorrelation function or this mutual information metric takes a long time to decay; autocorrelation for perfect Gaussian white noise is a sharp peak at zero lag, and zero everywhere else.&amp;#160;&lt;a class="footnote-backref" href="#fnref:memory" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:control"&gt;
&lt;p&gt;Compare to a non-chaotic signal, a perfect sinusoid + white noise, which looks "fuzzy" in phase space but the orbits always quickly return to the mean after deviation.&amp;#160;&lt;a class="footnote-backref" href="#fnref:control" rev="footnote" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:benettin"&gt;
&lt;p&gt;Benettin, Giancarlo, et al. "Lyapunov characteristic exponents for smooth dynamical systems and for Hamiltonian systems; a method for computing all of them. Part 1: Theory." Meccanica 15.1 (1980): 9-20.&amp;#160;&lt;a class="footnote-backref" href="#fnref:benettin" rev="footnote" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:rosenstein"&gt;
&lt;p&gt;Rosenstein, Michael T., James J. Collins, and Carlo J. De Luca. "A practical method for calculating largest Lyapunov exponents from small data sets." Physica D: Nonlinear Phenomena 65.1 (1993): 117-134.&amp;#160;&lt;a class="footnote-backref" href="#fnref:rosenstein" rev="footnote" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:computed"&gt;
&lt;p&gt;Computed from the differential equations, as opposed to &lt;em&gt;measured&lt;/em&gt; data from the LDV.&amp;#160;&lt;a class="footnote-backref" href="#fnref:computed" rev="footnote" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:error"&gt;
&lt;p&gt;The Rosenstein, et. al. paper has several tables documenting how their method behaves for various underlying attractors, embedding dimensions, number of data points, lag value, and so on.&amp;#160;&lt;a class="footnote-backref" href="#fnref:error" rev="footnote" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="chaos"></category><category term="geometry"></category><category term="research"></category><category term="experimental data"></category></entry><entry><title>Your Food is Always Outside of You</title><link href="http://blog.iank.org/your-food-is-always-outside-of-you.html" rel="alternate"></link><updated>2014-11-05T17:00:00-05:00</updated><author><name>Ian Kilgore</name></author><id>tag:blog.iank.org,2014-11-05:your-food-is-always-outside-of-you.html</id><summary type="html">&lt;iframe id="ytplayer" type="text/html" width="640" height="390" src="http://www.youtube.com/embed/OyTIqWk-O3E?autoplay=0&amp;origin=http://blog.iank.org" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;
&lt;/p&gt;

&lt;p&gt;Yesterday I gave a talk at the &lt;a href="http://lug.ncsu.edu"&gt;NCSU Linux Users' Group&lt;/a&gt; and I've posted the slides and a video here. Not captured in the audio was, after the talk, an impromptu lecture about Hilbert's infinite hotel and then &lt;a href="http://twitter.com/mambocab"&gt;Jim Witschey&lt;/a&gt; came up to talk about the expectation of the Poisson distribution. So it was a good time!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://iank.org/ncsulug_fa14.pdf"&gt;Slides (PDF)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://iank.org/ncsulug_fa14/ncsulug_fa14.html"&gt;References (HTML)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here's the original abstract of the talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;YOUR FOOD IS ALWAYS OUTSIDE OF YOU&lt;/p&gt;
&lt;p&gt;(Some Ideas About Space But Definitely Not Time)&lt;/p&gt;
&lt;p&gt;ABSTRACT:&lt;/p&gt;
&lt;p&gt;I'm going to, in an accessible way, cover some mathematical and physical ideas that I think are important or at least pretty cool. (CHILL. OUT.) You probably spent a lot of time in grade school factoring polynomials or whatever. I don't care about that. I want to talk about why orbits work, what happens in 5-D, why the World Series is slightly better than a coin toss, databases are broken forever, truth itself is wrong, and what happens if an infinite number of buses roll up at your house. Or some subset of that.&lt;/p&gt;
&lt;p&gt;I'll cover three or four discrete topics, so don't worry if you get lost; you'll be following along again in a few slides. Any equations will be supplementary only- you won't have to understand them to get the general idea.&lt;/p&gt;
&lt;p&gt;Here's what loudbot has to say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;lt; LOUDBOT&amp;gt; ik: GIVEN YOUR PAST PERFORMANCE THIS MAY QUALIFY AS A MIRACLE&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;</summary><category term="talks"></category><category term="physics"></category><category term="geometry"></category><category term="math"></category><category term="topology"></category></entry><entry><title>Inverse-Square Laws: A Physical Consequence of the Geometry of Space</title><link href="http://blog.iank.org/inverse-square-laws-a-physical-consequence-of-the-geometry-of-space.html" rel="alternate"></link><updated>2014-09-10T16:39:00-04:00</updated><author><name>Ian Kilgore</name></author><id>tag:blog.iank.org,2014-09-10:inverse-square-laws-a-physical-consequence-of-the-geometry-of-space.html</id><summary type="html">&lt;p&gt;John D. Barrow's &lt;a href="https://www.goodreads.com/book/show/18926355-the-constants-of-nature?ac=1"&gt;&lt;em&gt;The Constants of Nature&lt;/em&gt;&lt;/a&gt;&lt;sup id="fnref:barrow"&gt;&lt;a class="footnote-ref" href="#fn:barrow" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; mentions that Kant may have been the first to notice a connection between the dimensionality of space and physical inverse-square laws, such as &lt;a href="http://en.wikipedia.org/wiki/Newton's_law_of_universal_gravitation"&gt;Newton's law of universal gravitation&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
F = G\frac{m_1 m_2}{r^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;This is interesting. Inverse-square laws are &lt;a href="http://en.wikipedia.org/wiki/Inverse-square_law#Occurrences"&gt;everywhere&lt;/a&gt;, and 3D space really does appear to be special&lt;sup id="fnref:ehrenfest"&gt;&lt;a class="footnote-ref" href="#fn:ehrenfest" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;. Among other things, &lt;a href="http://en.wikipedia.org/wiki/Bertrand's_theorem"&gt;stable orbits depend on it&lt;/a&gt;. I first encountered, or at least noticed, the geometric reason for inverse-square laws in an electrodynamics and antenna theory lecture on &lt;a href="http://en.wikipedia.org/wiki/Free-space_path_loss"&gt;free-space path loss (FSPL)&lt;/a&gt;, which says that the power of a received signal in free space also has a $r^{-2}$ distance dependence.&lt;/p&gt;
&lt;p&gt;It is easy to understand this by picturing a point source radiating equally in all directions, i.e. spherically. Spherical symmetry is common in nature. At a distance $r$, the power is "spread out" over the surface of an imaginary sphere having area $4\pi r^2$. A similar argument can be made for gravity.&lt;/p&gt;
&lt;p&gt;This image (by &lt;a href="http://en.wikipedia.org/wiki/File:Inverse_square_law.svg"&gt;Borb&lt;/a&gt;, licensed under CC-BY-SA) helps illustrate the idea. At each distance $d$ the same total effect is distributed over an area $d^2$:&lt;/p&gt;
&lt;p&gt;&lt;img style="float: center" src="/images/500px-Inverse_square_law.svg.png" alt="Point source acting over a spherical area"&gt;&lt;/p&gt;
&lt;p&gt;This is clearly a consequence of the dimensionality of space as a sphere surface area scales with r^2 in 3-space. In general, an N-dimensional&lt;sup id="fnref:convention"&gt;&lt;a class="footnote-ref" href="#fn:convention" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; sphere has surface area which scales with $r^{N-1}$. So in 4D space, other things being equal&lt;sup id="fnref:equal"&gt;&lt;a class="footnote-ref" href="#fn:equal" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;, we would experience inverse-cube gravitation, electromagnetism, acoustics, and so on.&lt;/p&gt;
&lt;p&gt;Back to Kant. In his first published work (1747), "Thoughts on the true estimation of living forces"&lt;sup id="fnref:kant"&gt;&lt;a class="footnote-ref" href="#fn:kant" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, Kant argues (Section 9) that space would not exist "if substances had no forces to act external to themselves". This leads him in Section 10 to ague that 3-dimensional space is a consequence of inverse-square gravity. Kant concludes that the inverse-square law is arbitrary and that "God could have chosen another, e.g., the inverse-cube relation" and that this would have resulted in a different sort of space. He goes on to suggest, (well before the development of anything like modern topology or differential geometry), that these spatial possibilities ought to be investigated.&lt;/p&gt;
&lt;p&gt;It would seem that Kant got it backward, but Kant was like that.&lt;/p&gt;
&lt;p&gt;Nonetheless, these insights (and others&lt;sup id="fnref:misc"&gt;&lt;a class="footnote-ref" href="#fn:misc" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;) show that an important feature of many natural laws depends on "pure" geometrical truth moreso than physical reality; the inverse-square dependence is not due to an arbitrary constant exponent but the (admittedly, possibly arbitrary) dimensionality of space.&lt;/p&gt;
&lt;p&gt;The Greeks would be proud.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:barrow"&gt;
&lt;p&gt;Barrow, John D, "New Dimensions," in &lt;em&gt;The Constants of Nature&lt;/em&gt;. (New York: Random House, 2002), pp. 203-205&amp;#160;&lt;a class="footnote-backref" href="#fnref:barrow" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:kant"&gt;
&lt;p&gt;Kant, Immanuel, "Thoughts on the true estimation of living forces" in &lt;em&gt;Kant: Natural Science&lt;/em&gt;, ed. Eric Watkins. 1st ed. (Cambridge: Cambridge University Press, 2012). pp. 26-28. Cambridge Books Online. Web. 10 September 2014. http://dx.doi.org/10.1017/CBO9781139014380.004&amp;#160;&lt;a class="footnote-backref" href="#fnref:kant" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ehrenfest"&gt;
&lt;p&gt;Ehrenfest, Paul. "In what way does it become manifest in the fundamental laws of physics that space has three dimensions." Proc. Royal Netherlands Acad. Arts Sci 20 (1917): 200-209.&amp;#160;&lt;a class="footnote-backref" href="#fnref:ehrenfest" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:convention"&gt;
&lt;p&gt;By &lt;a href="http://mathworld.wolfram.com/Hypersphere.html"&gt;geometer's conventions&lt;/a&gt;. A topologist would call a 3D sphere a 2-sphere, as the surface has two dimensions.&amp;#160;&lt;a class="footnote-backref" href="#fnref:convention" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:equal"&gt;
&lt;p&gt;To the extent that it is possible for anything else to remain the same, i.e. "probably not."&amp;#160;&lt;a class="footnote-backref" href="#fnref:equal" rev="footnote" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:misc"&gt;
&lt;p&gt;2D and 3D spaces have many other special properties. See Ehrenfest (above) about rotation and wave propagation, Polya's work on the random walk on 2D, and Bertrand's Theorem.&amp;#160;&lt;a class="footnote-backref" href="#fnref:misc" rev="footnote" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="physics"></category><category term="geometry"></category><category term="kant"></category><category term="math"></category></entry></feed>